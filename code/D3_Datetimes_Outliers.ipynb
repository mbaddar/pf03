{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137f8a21",
   "metadata": {},
   "source": [
    "# Day 3: Datetimes & Outliers\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Parse datetime columns safely using `pd.to_datetime(..., errors=\"coerce\")`\n",
    "- Add time parts (date/month/day-of-week/hour) for grouping\n",
    "- Identify outliers using percentiles / IQR and choose a safe policy\n",
    "- Use core pandas ops for real work (`.loc`, `.assign`, `groupby/agg`)\n",
    "- Join tables safely with `merge(validate=...)` and detect join explosions\n",
    "\n",
    "**Run the two setup cells below before starting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffbd2c",
   "metadata": {},
   "source": [
    "## üë§ STEP 1: Enter Your Name (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "‚ùå Name too short! Edit the fields above and run this cell again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m _last \u001b[38;5;241m=\u001b[39m LAST_NAME\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_first) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_last) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Name too short! Edit the fields above and run this cell again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^[A-Za-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-]+$\u001b[39m\u001b[38;5;124m\"\u001b[39m, _first) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^[A-Za-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-]+$\u001b[39m\u001b[38;5;124m\"\u001b[39m, _last):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå English letters only! Edit the fields above and run this cell again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: ‚ùå Name too short! Edit the fields above and run this cell again."
     ]
    }
   ],
   "source": [
    "#@title üë§ STEP 1: Enter Your Name (REQUIRED)\n",
    "#@markdown Enter your name below and run this cell.\n",
    "\n",
    "FIRST_NAME = \"\"  #@param {type:\"string\"}\n",
    "LAST_NAME = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "FIRST_NAME = FIRST_NAME.strip() or input(\"First Name: \").strip()\n",
    "LAST_NAME = LAST_NAME.strip() or input(\"Last Name: \").strip()\n",
    "\n",
    "import re\n",
    "print(\"=\" * 40)\n",
    "_first = FIRST_NAME.strip()\n",
    "_last = LAST_NAME.strip()\n",
    "if len(_first) < 2 or len(_last) < 2:\n",
    "    raise ValueError(\"‚ùå Name too short! Edit the fields above and run this cell again.\")\n",
    "if not re.match(r\"^[A-Za-z\\s\\-]+$\", _first) or not re.match(r\"^[A-Za-z\\s\\-]+$\", _last):\n",
    "    raise ValueError(\"‚ùå English letters only! Edit the fields above and run this cell again.\")\n",
    "STUDENT_NAME = f\"{_first.upper()} {_last.upper()}\"\n",
    "print(f\"‚úÖ Welcome, {STUDENT_NAME}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe64a4f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è STEP 2: Setup (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eee0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è STEP 2: Setup (REQUIRED)\n",
    "#@markdown Run this cell to install dependencies and set up telemetry.\n",
    "\n",
    "try: STUDENT_NAME\n",
    "except NameError: raise ValueError(\"‚ùå Run the cell above first!\")\n",
    "\n",
    "# Install dependencies (works in Colab and local Jupyter)\n",
    "import subprocess\n",
    "import sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"posthog\"])\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import hashlib\n",
    "import time\n",
    "import uuid\n",
    "import posthog\n",
    "from functools import wraps\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# === TELEMETRY SETUP ===\n",
    "_H, _K = \"https://us.i.posthog.com\", \"phc_I7GaZ4p1Ox5PRM5egRrHFxz3ZCdh3zyKQ7B6jxJOWis\"\n",
    "_NB, _SESS = \"D3_Datetimes_Outliers\", uuid.uuid4().hex[:8]\n",
    "_SID = hashlib.md5(f\"{STUDENT_NAME}-bootcamp\".encode()).hexdigest()[:12]\n",
    "posthog.project_api_key, posthog.host = _K, _H\n",
    "_attempts = {}\n",
    "_cell_num = 0\n",
    "\n",
    "def _capture(e, p=None):\n",
    "    \"\"\"Send event to PostHog.\"\"\"\n",
    "    try: posthog.capture(_SID, e, {\"nb\": _NB, \"sess\": _SESS, \"student\": STUDENT_NAME, **(p or {})})\n",
    "    except: pass\n",
    "\n",
    "def _cell(cell_id, code_preview=\"\"):\n",
    "    \"\"\"Track cell execution. Call at START of every code cell.\"\"\"\n",
    "    global _cell_num\n",
    "    _cell_num += 1\n",
    "    _capture(\"cell\", {\"cell_id\": cell_id, \"cell_num\": _cell_num, \"code\": code_preview[:500]})\n",
    "\n",
    "def tracked_test(ex):\n",
    "    \"\"\"Decorator to track test function results.\"\"\"\n",
    "    def d(f):\n",
    "        @wraps(f)\n",
    "        def w(*a,**k):\n",
    "            _attempts[ex] = _attempts.get(ex,0)+1\n",
    "            t,ok,err = time.time(),False,None\n",
    "            try: r=f(*a,**k); ok=True; return r\n",
    "            except Exception as x: err=str(x)[:200]; raise\n",
    "            finally: _capture(\"test\", {\"ex\":ex,\"ok\":ok,\"try\":_attempts[ex],\"ms\":int((time.time()-t)*1000),\"err\":err})\n",
    "        return w\n",
    "    return d\n",
    "\n",
    "_capture(\"start\")\n",
    "print(f\"‚úÖ Ready! {STUDENT_NAME} | Session: {_SESS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebacab",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create sample `orders` and `users` DataFrames for the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517acc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate Sample Data\n",
    "_cell(\"generate_data\")\n",
    "\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample orders and users data for exercises.\"\"\"\n",
    "\n",
    "    # Generate users\n",
    "    n_users = 50\n",
    "    countries = [\"USA\", \"UK\", \"Germany\", \"France\", \"Japan\", \"Australia\"]\n",
    "    users = pd.DataFrame({\n",
    "        \"user_id\": [f\"U{i:04d}\" for i in range(1, n_users + 1)],\n",
    "        \"country\": np.random.choice(countries, n_users),\n",
    "        \"signup_date\": pd.date_range(\"2024-01-01\", periods=n_users, freq=\"3D\").astype(str),\n",
    "    })\n",
    "\n",
    "    # Generate orders\n",
    "    n_orders = 500\n",
    "    statuses = [\"paid\", \"paid\", \"paid\", \"paid\", \"refund\", \"pending\"]  # weighted\n",
    "\n",
    "    # Create some intentionally messy datetime formats\n",
    "    base_dates = pd.date_range(\"2024-06-01\", periods=n_orders, freq=\"2h\")\n",
    "    datetime_formats = [\n",
    "        lambda d: d.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        lambda d: d.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        lambda d: d.strftime(\"%d/%m/%Y %H:%M\"),\n",
    "        lambda d: \"invalid_date\",  # intentionally invalid\n",
    "    ]\n",
    "\n",
    "    created_at = []\n",
    "    for i, d in enumerate(base_dates):\n",
    "        if i % 50 == 0:  # 2% invalid dates\n",
    "            created_at.append(\"invalid_date\")\n",
    "        else:\n",
    "            fmt = datetime_formats[i % 3]\n",
    "            created_at.append(fmt(d))\n",
    "\n",
    "    # Generate amounts with some outliers\n",
    "    amounts = np.random.lognormal(mean=3.5, sigma=0.8, size=n_orders)\n",
    "    # Add some extreme outliers\n",
    "    outlier_indices = np.random.choice(n_orders, size=10, replace=False)\n",
    "    amounts[outlier_indices] = np.random.uniform(5000, 20000, size=10)\n",
    "\n",
    "    orders = pd.DataFrame({\n",
    "        \"order_id\": [f\"ORD{i:05d}\" for i in range(1, n_orders + 1)],\n",
    "        \"user_id\": np.random.choice(users[\"user_id\"].tolist(), n_orders),\n",
    "        \"amount\": amounts.round(2),\n",
    "        \"quantity\": np.random.randint(1, 10, n_orders),\n",
    "        \"created_at\": created_at,\n",
    "        \"status_clean\": np.random.choice(statuses, n_orders),\n",
    "    })\n",
    "\n",
    "    return orders, users\n",
    "\n",
    "orders, users = generate_sample_data()\n",
    "print(f\"‚úÖ Generated {len(orders)} orders and {len(users)} users\")\n",
    "print(f\"\\nOrders sample:\\n{orders.head()}\")\n",
    "print(f\"\\nUsers sample:\\n{users.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b83f6",
   "metadata": {},
   "source": [
    "---\n",
    "# Session 1: Datetime Parsing & Time Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f49b8",
   "metadata": {},
   "source": [
    "## Concept: Timestamps vs Strings\n",
    "\n",
    "A timestamp should be:\n",
    "- A real datetime type (`datetime64[ns]`, often UTC-aware)\n",
    "- Sortable by time\n",
    "- Groupable by month/week/day\n",
    "\n",
    "A string \"looks like time\" but behaves like text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd6085",
   "metadata": {},
   "source": [
    "## Helper Function: `parse_datetime`\n",
    "\n",
    "Use `errors=\"coerce\"` to safely parse datetimes - invalid formats become NaT (missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d241a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define: parse_datetime\n",
    "_cell(\"helper_parse_datetime\")\n",
    "\n",
    "def parse_datetime(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    *,\n",
    "    utc: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse a timestamp column safely.\n",
    "    - invalid strings become NA (errors=\"coerce\")\n",
    "    - utc=True gives timezone-aware timestamps (recommended default)\n",
    "    \"\"\"\n",
    "    dt = pd.to_datetime(df[col], errors=\"coerce\", utc=utc)\n",
    "    return df.assign(**{col: dt})\n",
    "\n",
    "print(\"‚úÖ parse_datetime defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b76580",
   "metadata": {},
   "source": [
    "## Exercise 1: Parse `created_at`\n",
    "\n",
    "**Task:**\n",
    "1. Parse the `created_at` column using `parse_datetime`\n",
    "2. Count how many values became NaT (missing) after parsing\n",
    "3. Verify the dtype changed to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeca853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 1: Parse created_at\n",
    "_cell(\"ex1_parse_created_at\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: Use parse_datetime(orders, \"created_at\", utc=True)\n",
    "\n",
    "orders_parsed = parse_datetime(orders.copy(), \"created_at\", utc=True)\n",
    "\n",
    "# Check your work\n",
    "print(f\"Original dtype: {orders['created_at'].dtype}\")\n",
    "print(f\"Parsed dtype: {orders_parsed['created_at'].dtype}\")\n",
    "print(f\"Missing after parse: {orders_parsed['created_at'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32812cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 1\n",
    "_cell(\"test_ex1\")\n",
    "\n",
    "@tracked_test(\"parse_datetime\")\n",
    "def test_parse_datetime():\n",
    "    \"\"\"Test that parse_datetime works correctly.\"\"\"\n",
    "    # Create test data\n",
    "    test_df = pd.DataFrame({\n",
    "        \"timestamp\": [\"2024-01-01 10:00:00\", \"invalid\", \"2024-01-02 11:00:00\"]\n",
    "    })\n",
    "\n",
    "    result = parse_datetime(test_df, \"timestamp\", utc=True)\n",
    "\n",
    "    # Test 1: dtype should be datetime\n",
    "    assert pd.api.types.is_datetime64_any_dtype(result[\"timestamp\"]), \\\n",
    "        \"Column should be datetime type\"\n",
    "\n",
    "    # Test 2: invalid should become NaT\n",
    "    assert result[\"timestamp\"].isna().sum() == 1, \\\n",
    "        \"Invalid dates should become NaT\"\n",
    "\n",
    "    # Test 3: valid dates should parse correctly\n",
    "    assert result[\"timestamp\"].dropna().iloc[0].year == 2024, \\\n",
    "        \"Valid dates should parse correctly\"\n",
    "\n",
    "    print(\"‚úÖ test_parse_datetime passed!\")\n",
    "\n",
    "test_parse_datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a31447",
   "metadata": {},
   "source": [
    "## Helper Function: `add_time_parts`\n",
    "\n",
    "Add common grouping keys from a timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define: add_time_parts\n",
    "_cell(\"helper_add_time_parts\")\n",
    "\n",
    "def add_time_parts(df: pd.DataFrame, ts_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Add common time grouping keys (month, day-of-week, hour, etc.).\"\"\"\n",
    "    ts = df[ts_col]\n",
    "    return df.assign(\n",
    "        date=ts.dt.date,\n",
    "        year=ts.dt.year,\n",
    "        month=ts.dt.to_period(\"M\").astype(\"string\"),\n",
    "        dow=ts.dt.day_name(),\n",
    "        hour=ts.dt.hour,\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ add_time_parts defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce1bbe",
   "metadata": {},
   "source": [
    "## Exercise 2: Add Time Parts\n",
    "\n",
    "**Task:**\n",
    "1. Parse `created_at` first\n",
    "2. Add time parts using `add_time_parts`\n",
    "3. Print the first 5 rows showing: `created_at`, `month`, `dow`, `hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d5ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 2: Add Time Parts\n",
    "_cell(\"ex2_add_time_parts\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "orders_with_time = (\n",
    "    orders.copy()\n",
    "    .pipe(parse_datetime, col=\"created_at\", utc=True)\n",
    "    .pipe(add_time_parts, ts_col=\"created_at\")\n",
    ")\n",
    "\n",
    "print(orders_with_time[[\"created_at\", \"month\", \"dow\", \"hour\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78191777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 2\n",
    "_cell(\"test_ex2\")\n",
    "\n",
    "@tracked_test(\"add_time_parts\")\n",
    "def test_add_time_parts():\n",
    "    \"\"\"Test that add_time_parts creates correct columns.\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        \"ts\": pd.to_datetime([\"2024-03-15 14:30:00\", \"2024-07-20 09:00:00\"], utc=True)\n",
    "    })\n",
    "\n",
    "    result = add_time_parts(test_df, \"ts\")\n",
    "\n",
    "    # Test 1: Required columns exist\n",
    "    required_cols = [\"date\", \"year\", \"month\", \"dow\", \"hour\"]\n",
    "    for col in required_cols:\n",
    "        assert col in result.columns, f\"Missing column: {col}\"\n",
    "\n",
    "    # Test 2: Values are correct\n",
    "    assert result[\"year\"].iloc[0] == 2024, \"Year should be 2024\"\n",
    "    assert result[\"hour\"].iloc[0] == 14, \"Hour should be 14\"\n",
    "    assert result[\"dow\"].iloc[0] == \"Friday\", \"March 15, 2024 is Friday\"\n",
    "\n",
    "    # Test 3: Month format\n",
    "    assert \"2024-03\" in result[\"month\"].iloc[0], \"Month should be in YYYY-MM format\"\n",
    "\n",
    "    print(\"‚úÖ test_add_time_parts passed!\")\n",
    "\n",
    "test_add_time_parts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3ed75",
   "metadata": {},
   "source": [
    "## Quick Check\n",
    "\n",
    "**Question:** What does `errors=\"coerce\"` do?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "Invalid datetime strings become NaT (missing values) instead of raising an exception.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29030373",
   "metadata": {},
   "source": [
    "---\n",
    "# Session 2: Outliers & Core Pandas Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de37ae",
   "metadata": {},
   "source": [
    "## Concept: Percentiles\n",
    "\n",
    "Percentiles help understand the distribution:\n",
    "- **p50 (median)**: \"typical\" value\n",
    "- **p90**: 90% of values are below this\n",
    "- **p99**: 99% of values are below this (only 1% are above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a34dae",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute Percentiles\n",
    "\n",
    "**Task:**\n",
    "1. Compute p50, p90, p99 for the `amount` column\n",
    "2. Identify if there are extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 3: Compute Percentiles\n",
    "_cell(\"ex3_percentiles\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "s = orders[\"amount\"].dropna()\n",
    "percentiles = s.quantile([0.50, 0.90, 0.99])\n",
    "print(\"Percentiles for 'amount':\")\n",
    "print(percentiles)\n",
    "print(f\"\\nMax value: {s.max():.2f}\")\n",
    "print(f\"If p99 is much smaller than max, you have extreme outliers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ec341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 3\n",
    "_cell(\"test_ex3\")\n",
    "\n",
    "@tracked_test(\"percentiles\")\n",
    "def test_percentiles():\n",
    "    \"\"\"Test understanding of percentiles.\"\"\"\n",
    "    test_s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 100])  # 100 is an outlier\n",
    "\n",
    "    p50 = test_s.quantile(0.50)\n",
    "    p99 = test_s.quantile(0.99)\n",
    "\n",
    "    # Test 1: Median should be around 5.5\n",
    "    assert 5 <= p50 <= 6, f\"p50 should be between 5 and 6, got {p50}\"\n",
    "\n",
    "    # Test 2: p99 should be close to the outlier\n",
    "    assert p99 > 50, f\"p99 should capture the outlier region, got {p99}\"\n",
    "\n",
    "    # Test 3: Mean vs Median for outlier detection\n",
    "    mean = test_s.mean()\n",
    "    assert mean > p50, \"Mean should be > median when there are positive outliers\"\n",
    "\n",
    "    print(\"‚úÖ test_percentiles passed!\")\n",
    "\n",
    "test_percentiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19628c",
   "metadata": {},
   "source": [
    "## Helper Function: `iqr_bounds`\n",
    "\n",
    "IQR method for outlier detection:\n",
    "- Q1 = 25th percentile\n",
    "- Q3 = 75th percentile\n",
    "- IQR = Q3 - Q1\n",
    "- Outliers are outside `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5211af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define: iqr_bounds\n",
    "_cell(\"helper_iqr_bounds\")\n",
    "\n",
    "def iqr_bounds(s: pd.Series, k: float = 1.5) -> tuple[float, float]:\n",
    "    \"\"\"Return (lo, hi) IQR bounds for outlier flagging.\"\"\"\n",
    "    x = s.dropna()\n",
    "    q1 = x.quantile(0.25)\n",
    "    q3 = x.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    return float(q1 - k * iqr), float(q3 + k * iqr)\n",
    "\n",
    "print(\"‚úÖ iqr_bounds defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe796e4",
   "metadata": {},
   "source": [
    "## Exercise 4: Count Outliers\n",
    "\n",
    "**Task:**\n",
    "1. Compute IQR bounds for `amount`\n",
    "2. Count how many values are outside the bounds\n",
    "3. What percentage of orders are outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bbdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 4: Count Outliers\n",
    "_cell(\"ex4_count_outliers\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "lo, hi = iqr_bounds(orders[\"amount\"], k=1.5)\n",
    "print(f\"IQR bounds: [{lo:.2f}, {hi:.2f}]\")\n",
    "\n",
    "n_outliers = ((orders[\"amount\"] < lo) | (orders[\"amount\"] > hi)).sum()\n",
    "pct_outliers = n_outliers / len(orders) * 100\n",
    "print(f\"Outliers: {n_outliers} ({pct_outliers:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 4\n",
    "_cell(\"test_ex4\")\n",
    "\n",
    "@tracked_test(\"iqr_bounds\")\n",
    "def test_iqr_bounds():\n",
    "    \"\"\"Test IQR bounds calculation.\"\"\"\n",
    "    # Create data with known quartiles\n",
    "    test_s = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "    lo, hi = iqr_bounds(test_s, k=1.5)\n",
    "\n",
    "    # For this data: Q1=3.25, Q3=7.75, IQR=4.5\n",
    "    # lo = 3.25 - 1.5*4.5 = -3.5\n",
    "    # hi = 7.75 + 1.5*4.5 = 14.5\n",
    "\n",
    "    assert lo < 0, f\"Lower bound should be negative, got {lo}\"\n",
    "    assert hi > 10, f\"Upper bound should be > 10, got {hi}\"\n",
    "\n",
    "    # All values should be within bounds\n",
    "    outliers = (test_s < lo) | (test_s > hi)\n",
    "    assert outliers.sum() == 0, \"No outliers expected in this data\"\n",
    "\n",
    "    print(\"‚úÖ test_iqr_bounds passed!\")\n",
    "\n",
    "test_iqr_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195f4f9",
   "metadata": {},
   "source": [
    "## Helper Function: `winsorize`\n",
    "\n",
    "Cap extreme values for visualization (don't delete rows!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414cf410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define: winsorize\n",
    "_cell(\"helper_winsorize\")\n",
    "\n",
    "def winsorize(s: pd.Series, lo: float = 0.01, hi: float = 0.99) -> pd.Series:\n",
    "    \"\"\"Cap values to [p_lo, p_hi] (helpful for visualization, not deletion).\"\"\"\n",
    "    x = s.dropna()\n",
    "    a, b = x.quantile(lo), x.quantile(hi)\n",
    "    return s.clip(lower=a, upper=b)\n",
    "\n",
    "print(\"‚úÖ winsorize defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61802e85",
   "metadata": {},
   "source": [
    "## Exercise 5: Create Winsorized Amount\n",
    "\n",
    "**Task:**\n",
    "1. Create a new column `amount_winsor` using the winsorize function\n",
    "2. Compare min/max of original vs winsorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 5: Create Winsorized Amount\n",
    "_cell(\"ex5_winsorize\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "orders_w = orders.assign(amount_winsor=winsorize(orders[\"amount\"]))\n",
    "\n",
    "print(\"Original amount:\")\n",
    "print(f\"  Min: {orders['amount'].min():.2f}, Max: {orders['amount'].max():.2f}\")\n",
    "print(f\"\\nWinsorized amount (1%-99%):\")\n",
    "print(f\"  Min: {orders_w['amount_winsor'].min():.2f}, Max: {orders_w['amount_winsor'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8263a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 5\n",
    "_cell(\"test_ex5\")\n",
    "\n",
    "@tracked_test(\"winsorize\")\n",
    "def test_winsorize():\n",
    "    \"\"\"Test winsorization.\"\"\"\n",
    "    test_s = pd.Series([1, 2, 3, 4, 5, 100, 200, 300])  # extreme outliers\n",
    "\n",
    "    result = winsorize(test_s, lo=0.1, hi=0.9)\n",
    "\n",
    "    # Test 1: Same length\n",
    "    assert len(result) == len(test_s), \"Length should not change\"\n",
    "\n",
    "    # Test 2: Extreme values should be capped\n",
    "    assert result.max() < test_s.max(), \"Max should be reduced\"\n",
    "\n",
    "    # Test 3: Middle values should not change\n",
    "    # (not strictly true due to clipping, but outliers definitely should change)\n",
    "    assert result.iloc[-1] < 300, \"Extreme value 300 should be capped\"\n",
    "\n",
    "    print(\"‚úÖ test_winsorize passed!\")\n",
    "\n",
    "test_winsorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6bc063",
   "metadata": {},
   "source": [
    "## Core Pandas Pattern 1: Selection with `.loc`\n",
    "\n",
    "Avoid chained indexing. Use `.loc[row_condition, columns]` for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cecb34",
   "metadata": {},
   "source": [
    "## Exercise 6: Filter Paid Orders\n",
    "\n",
    "**Task:**\n",
    "1. Filter rows where `status_clean == \"paid\"`\n",
    "2. Keep only `user_id` and `amount` columns\n",
    "3. Count the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 6: Filter Paid Orders\n",
    "_cell(\"ex6_filter_paid\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "paid_orders = orders.loc[\n",
    "    orders[\"status_clean\"] == \"paid\",\n",
    "    [\"user_id\", \"amount\"]\n",
    "]\n",
    "print(f\"Paid orders: {len(paid_orders)}\")\n",
    "print(f\"Total revenue from paid orders: ${paid_orders['amount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa96565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 6\n",
    "_cell(\"test_ex6\")\n",
    "\n",
    "@tracked_test(\"filter_paid\")\n",
    "def test_filter_paid():\n",
    "    \"\"\"Test filtering with .loc.\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        \"status\": [\"paid\", \"refund\", \"paid\", \"pending\"],\n",
    "        \"amount\": [100, 50, 200, 75],\n",
    "        \"user\": [\"A\", \"B\", \"C\", \"D\"]\n",
    "    })\n",
    "\n",
    "    result = test_df.loc[test_df[\"status\"] == \"paid\", [\"user\", \"amount\"]]\n",
    "\n",
    "    # Test 1: Correct number of rows\n",
    "    assert len(result) == 2, f\"Should have 2 paid orders, got {len(result)}\"\n",
    "\n",
    "    # Test 2: Correct columns\n",
    "    assert list(result.columns) == [\"user\", \"amount\"], \"Should have user and amount columns\"\n",
    "\n",
    "    # Test 3: Correct values\n",
    "    assert result[\"amount\"].sum() == 300, \"Sum of paid amounts should be 300\"\n",
    "\n",
    "    print(\"‚úÖ test_filter_paid passed!\")\n",
    "\n",
    "test_filter_paid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffc85c",
   "metadata": {},
   "source": [
    "## Core Pandas Pattern 2: Assignment with `.assign`\n",
    "\n",
    "Use `.assign()` to add columns without modifying the original DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d5086",
   "metadata": {},
   "source": [
    "## Exercise 7: Add Boolean Column\n",
    "\n",
    "**Task:**\n",
    "Add a column `is_refund` that is True when `status_clean == \"refund\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 7: Add Boolean Column\n",
    "_cell(\"ex7_add_boolean\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "orders_with_refund = orders.assign(\n",
    "    is_refund=orders[\"status_clean\"] == \"refund\"\n",
    ")\n",
    "print(orders_with_refund[\"is_refund\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c90667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 7\n",
    "_cell(\"test_ex7\")\n",
    "\n",
    "@tracked_test(\"add_boolean\")\n",
    "def test_add_boolean():\n",
    "    \"\"\"Test adding boolean column with .assign.\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        \"status\": [\"paid\", \"refund\", \"paid\", \"refund\"]\n",
    "    })\n",
    "\n",
    "    result = test_df.assign(is_refund=test_df[\"status\"] == \"refund\")\n",
    "\n",
    "    # Test 1: Column exists\n",
    "    assert \"is_refund\" in result.columns, \"is_refund column should exist\"\n",
    "\n",
    "    # Test 2: Correct dtype\n",
    "    assert result[\"is_refund\"].dtype == bool, \"Should be boolean dtype\"\n",
    "\n",
    "    # Test 3: Correct values\n",
    "    assert result[\"is_refund\"].sum() == 2, \"Should have 2 refunds\"\n",
    "\n",
    "    print(\"‚úÖ test_add_boolean passed!\")\n",
    "\n",
    "test_add_boolean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf8a0e",
   "metadata": {},
   "source": [
    "## Core Pandas Pattern 3: GroupBy/Agg\n",
    "\n",
    "Most analytics tables come from:\n",
    "- `groupby` ‚Üí `agg` ‚Üí `reset_index`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df44f5",
   "metadata": {},
   "source": [
    "## Exercise 8: Revenue Per User\n",
    "\n",
    "**Task:**\n",
    "Compute per-user summary with:\n",
    "- `n_orders`: count of orders\n",
    "- `revenue`: sum of amount\n",
    "- `aov`: average order value (mean of amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e462e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 8: Revenue Per User\n",
    "_cell(\"ex8_revenue_per_user\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "per_user = (\n",
    "    orders.groupby(\"user_id\", dropna=False)\n",
    "    .agg(\n",
    "        n_orders=(\"order_id\", \"size\"),\n",
    "        revenue=(\"amount\", \"sum\"),\n",
    "        aov=(\"amount\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"revenue\", ascending=False)\n",
    ")\n",
    "print(f\"Top 5 users by revenue:\\n{per_user.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 8\n",
    "_cell(\"test_ex8\")\n",
    "\n",
    "@tracked_test(\"revenue_per_user\")\n",
    "def test_revenue_per_user():\n",
    "    \"\"\"Test groupby aggregation.\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        \"user_id\": [\"A\", \"A\", \"B\", \"B\", \"B\"],\n",
    "        \"order_id\": [1, 2, 3, 4, 5],\n",
    "        \"amount\": [100, 200, 50, 50, 100]\n",
    "    })\n",
    "\n",
    "    result = (\n",
    "        test_df.groupby(\"user_id\")\n",
    "        .agg(\n",
    "            n_orders=(\"order_id\", \"size\"),\n",
    "            revenue=(\"amount\", \"sum\"),\n",
    "            aov=(\"amount\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Test 1: Correct number of users\n",
    "    assert len(result) == 2, \"Should have 2 users\"\n",
    "\n",
    "    # Test 2: Correct order counts\n",
    "    user_a = result[result[\"user_id\"] == \"A\"].iloc[0]\n",
    "    assert user_a[\"n_orders\"] == 2, \"User A should have 2 orders\"\n",
    "\n",
    "    # Test 3: Correct revenue\n",
    "    assert user_a[\"revenue\"] == 300, \"User A revenue should be 300\"\n",
    "\n",
    "    # Test 4: Correct AOV\n",
    "    assert user_a[\"aov\"] == 150, \"User A AOV should be 150\"\n",
    "\n",
    "    print(\"‚úÖ test_revenue_per_user passed!\")\n",
    "\n",
    "test_revenue_per_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be217d68",
   "metadata": {},
   "source": [
    "## Exercise 9: Refund Rate by Status\n",
    "\n",
    "**Task:**\n",
    "Calculate the refund rate:\n",
    "- numerator: number of refunds\n",
    "- denominator: total orders\n",
    "- rate: refunds / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 9: Refund Rate\n",
    "_cell(\"ex9_refund_rate\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "refund_summary = (\n",
    "    orders.assign(is_refund=orders[\"status_clean\"] == \"refund\")\n",
    "    .agg(\n",
    "        total_orders=(\"order_id\", \"size\"),\n",
    "        refunds=(\"is_refund\", \"sum\"),\n",
    "    )\n",
    ")\n",
    "refund_rate = refund_summary[\"refunds\"] / refund_summary[\"total_orders\"]\n",
    "print(f\"Total orders: {refund_summary['total_orders']}\")\n",
    "print(f\"Refunds: {refund_summary['refunds']}\")\n",
    "print(f\"Refund rate: {refund_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83020512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 9\n",
    "_cell(\"test_ex9\")\n",
    "\n",
    "@tracked_test(\"refund_rate\")\n",
    "def test_refund_rate():\n",
    "    \"\"\"Test rate calculation.\"\"\"\n",
    "    test_df = pd.DataFrame({\n",
    "        \"order_id\": range(10),\n",
    "        \"status\": [\"paid\"] * 8 + [\"refund\"] * 2\n",
    "    })\n",
    "\n",
    "    n_refunds = (test_df[\"status\"] == \"refund\").sum()\n",
    "    n_total = len(test_df)\n",
    "    rate = n_refunds / n_total\n",
    "\n",
    "    # Test 1: Correct refund count\n",
    "    assert n_refunds == 2, \"Should have 2 refunds\"\n",
    "\n",
    "    # Test 2: Correct total\n",
    "    assert n_total == 10, \"Should have 10 orders\"\n",
    "\n",
    "    # Test 3: Correct rate\n",
    "    assert rate == 0.2, \"Refund rate should be 20%\"\n",
    "\n",
    "    print(\"‚úÖ test_refund_rate passed!\")\n",
    "\n",
    "test_refund_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9285e",
   "metadata": {},
   "source": [
    "---\n",
    "# Session 3: Safe Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c8ab2",
   "metadata": {},
   "source": [
    "## Concept: Join Types\n",
    "\n",
    "- **Left join**: Keep all rows from left table, enrich with right (most common)\n",
    "- **Inner join**: Keep only matches (can silently drop data!)\n",
    "- **Outer join**: Keep all rows from both sides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c185a",
   "metadata": {},
   "source": [
    "## Helper Function: `safe_left_join`\n",
    "\n",
    "Use `validate` parameter to prevent join explosions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1accaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define: safe_left_join\n",
    "_cell(\"helper_safe_left_join\")\n",
    "\n",
    "def safe_left_join(\n",
    "    left: pd.DataFrame,\n",
    "    right: pd.DataFrame,\n",
    "    on: str | list[str],\n",
    "    *,\n",
    "    validate: str,\n",
    "    suffixes: tuple[str, str] = (\"\", \"_r\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-join with enforced join cardinality via `validate=`.\n",
    "\n",
    "    validate options:\n",
    "    - \"many_to_one\": left may repeat keys; right must be unique\n",
    "    - \"one_to_one\": both sides unique\n",
    "    - \"many_to_many\": allowed, but dangerous!\n",
    "    \"\"\"\n",
    "    return left.merge(\n",
    "        right,\n",
    "        how=\"left\",\n",
    "        on=on,\n",
    "        validate=validate,\n",
    "        suffixes=suffixes,\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ safe_left_join defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b8219",
   "metadata": {},
   "source": [
    "## Helper Function: `assert_unique_key`\n",
    "\n",
    "Always check that lookup tables have unique keys before joining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define: assert_unique_key\n",
    "_cell(\"helper_assert_unique_key\")\n",
    "\n",
    "def assert_unique_key(df: pd.DataFrame, key: str | list[str]) -> None:\n",
    "    \"\"\"Assert that key column(s) are unique in the DataFrame.\"\"\"\n",
    "    if isinstance(key, str):\n",
    "        key = [key]\n",
    "\n",
    "    is_unique = not df.duplicated(subset=key).any()\n",
    "    if not is_unique:\n",
    "        n_dups = df.duplicated(subset=key).sum()\n",
    "        raise AssertionError(f\"Key {key} is not unique: {n_dups} duplicates found\")\n",
    "\n",
    "print(\"‚úÖ assert_unique_key defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd88a9",
   "metadata": {},
   "source": [
    "## Exercise 10: Safe Join Orders to Users\n",
    "\n",
    "**Task:**\n",
    "1. Check that `user_id` is unique in users table\n",
    "2. Join orders (left) with users (right) on `user_id`\n",
    "3. Verify row count didn't change (no join explosion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 10: Safe Join\n",
    "_cell(\"ex10_safe_join\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Check uniqueness\n",
    "assert_unique_key(users, \"user_id\")\n",
    "print(\"‚úÖ user_id is unique in users table\")\n",
    "\n",
    "# Step 2: Join\n",
    "joined = safe_left_join(\n",
    "    orders,\n",
    "    users,\n",
    "    on=\"user_id\",\n",
    "    validate=\"many_to_one\",\n",
    "    suffixes=(\"\", \"_user\")\n",
    ")\n",
    "\n",
    "# Step 3: Verify row count\n",
    "assert len(joined) == len(orders), \"Row count changed - possible join explosion!\"\n",
    "print(f\"‚úÖ Row count preserved: {len(orders)} ‚Üí {len(joined)}\")\n",
    "\n",
    "# Check match rate\n",
    "match_rate = 1 - joined[\"country\"].isna().mean()\n",
    "print(f\"‚úÖ Country match rate: {match_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d04277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 10\n",
    "_cell(\"test_ex10\")\n",
    "\n",
    "@tracked_test(\"safe_join\")\n",
    "def test_safe_join():\n",
    "    \"\"\"Test safe left join.\"\"\"\n",
    "    # Create test data\n",
    "    left = pd.DataFrame({\n",
    "        \"id\": [1, 2, 3, 1],  # 1 appears twice\n",
    "        \"value\": [10, 20, 30, 40]\n",
    "    })\n",
    "\n",
    "    right_unique = pd.DataFrame({\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"name\": [\"A\", \"B\", \"C\"]\n",
    "    })\n",
    "\n",
    "    right_duplicate = pd.DataFrame({\n",
    "        \"id\": [1, 1, 2, 3],  # 1 appears twice - will cause explosion!\n",
    "        \"name\": [\"A1\", \"A2\", \"B\", \"C\"]\n",
    "    })\n",
    "\n",
    "    # Test 1: Valid many_to_one join\n",
    "    result = safe_left_join(left, right_unique, on=\"id\", validate=\"many_to_one\")\n",
    "    assert len(result) == len(left), \"Row count should be preserved for many_to_one\"\n",
    "\n",
    "    # Test 2: Join explosion should raise error\n",
    "    try:\n",
    "        safe_left_join(left, right_duplicate, on=\"id\", validate=\"many_to_one\")\n",
    "        assert False, \"Should have raised an error for duplicate keys\"\n",
    "    except Exception as e:\n",
    "        assert \"MergeError\" in type(e).__name__ or \"merge\" in str(e).lower(), \\\n",
    "            \"Should raise merge validation error\"\n",
    "\n",
    "    # Test 3: Columns from right are added\n",
    "    assert \"name\" in result.columns, \"Right table columns should be added\"\n",
    "\n",
    "    print(\"‚úÖ test_safe_join passed!\")\n",
    "\n",
    "test_safe_join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11293ce",
   "metadata": {},
   "source": [
    "## Exercise 11: Detect dtype Mismatch\n",
    "\n",
    "**Question:** Which join keys will match?\n",
    "\n",
    "A) left has `\"001\"`, right has `\"001\"` (both strings)\n",
    "B) left has `\"001\"`, right has `1` (string vs int)\n",
    "C) left has `1`, right has `1` (both int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6180f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 11: Detect dtype Mismatch\n",
    "_cell(\"ex11_dtype_mismatch\")\n",
    "\n",
    "# YOUR CODE HERE - Test each scenario\n",
    "\n",
    "# Scenario A: string == string\n",
    "left_a = pd.DataFrame({\"id\": [\"001\", \"002\"], \"val\": [1, 2]})\n",
    "right_a = pd.DataFrame({\"id\": [\"001\", \"003\"], \"name\": [\"A\", \"C\"]})\n",
    "merged_a = left_a.merge(right_a, on=\"id\", how=\"left\")\n",
    "print(\"A) string-string match rate:\", merged_a[\"name\"].notna().mean())\n",
    "\n",
    "# Scenario B: string != int\n",
    "left_b = pd.DataFrame({\"id\": [\"001\", \"002\"], \"val\": [1, 2]})\n",
    "right_b = pd.DataFrame({\"id\": [1, 3], \"name\": [\"A\", \"C\"]})\n",
    "merged_b = left_b.merge(right_b, on=\"id\", how=\"left\")\n",
    "print(\"B) string-int match rate:\", merged_b[\"name\"].notna().mean())\n",
    "\n",
    "# Scenario C: int == int\n",
    "left_c = pd.DataFrame({\"id\": [1, 2], \"val\": [1, 2]})\n",
    "right_c = pd.DataFrame({\"id\": [1, 3], \"name\": [\"A\", \"C\"]})\n",
    "merged_c = left_c.merge(right_c, on=\"id\", how=\"left\")\n",
    "print(\"C) int-int match rate:\", merged_c[\"name\"].notna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 11\n",
    "_cell(\"test_ex11\")\n",
    "\n",
    "@tracked_test(\"dtype_match\")\n",
    "def test_dtype_match():\n",
    "    \"\"\"Test dtype matching in joins.\"\"\"\n",
    "    left_str = pd.DataFrame({\"id\": [\"001\"], \"x\": [1]})\n",
    "    right_str = pd.DataFrame({\"id\": [\"001\"], \"y\": [2]})\n",
    "    right_int = pd.DataFrame({\"id\": [1], \"y\": [2]})\n",
    "\n",
    "    # String-string should match\n",
    "    result1 = left_str.merge(right_str, on=\"id\", how=\"left\")\n",
    "    assert result1[\"y\"].notna().all(), \"String-string should match\"\n",
    "\n",
    "    # String-int should NOT match (in most cases)\n",
    "    result2 = left_str.merge(right_int, on=\"id\", how=\"left\")\n",
    "    assert result2[\"y\"].isna().all(), \"String-int should not match\"\n",
    "\n",
    "    print(\"‚úÖ test_dtype_match passed!\")\n",
    "\n",
    "test_dtype_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01818880",
   "metadata": {},
   "source": [
    "## Exercise 12: Build Complete Analytics Table\n",
    "\n",
    "**Task:**\n",
    "Build a complete analytics-ready table by:\n",
    "1. Parsing `created_at`\n",
    "2. Adding time parts\n",
    "3. Joining with users\n",
    "4. Adding winsorized amount\n",
    "5. Adding outlier flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Exercise 12: Build Analytics Table\n",
    "_cell(\"ex12_analytics_table\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def add_outlier_flag(df: pd.DataFrame, col: str, *, k: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"Add a boolean flag for outliers based on IQR.\"\"\"\n",
    "    lo, hi = iqr_bounds(df[col], k=k)\n",
    "    return df.assign(**{f\"{col}__is_outlier\": (df[col] < lo) | (df[col] > hi)})\n",
    "\n",
    "# Build the analytics table step by step\n",
    "analytics = (\n",
    "    orders.copy()\n",
    "    .pipe(parse_datetime, col=\"created_at\", utc=True)\n",
    "    .pipe(add_time_parts, ts_col=\"created_at\")\n",
    "    .pipe(safe_left_join, users, on=\"user_id\", validate=\"many_to_one\", suffixes=(\"\", \"_user\"))\n",
    "    .assign(amount_winsor=lambda d: winsorize(d[\"amount\"]))\n",
    "    .pipe(add_outlier_flag, col=\"amount\", k=1.5)\n",
    ")\n",
    "\n",
    "print(\"Analytics table columns:\")\n",
    "print(analytics.columns.tolist())\n",
    "print(f\"\\nShape: {analytics.shape}\")\n",
    "print(f\"\\nSample:\\n{analytics[['order_id', 'user_id', 'country', 'month', 'amount', 'amount_winsor', 'amount__is_outlier']].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfab7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Exercise 12\n",
    "_cell(\"test_ex12\")\n",
    "\n",
    "@tracked_test(\"analytics_table\")\n",
    "def test_analytics_table():\n",
    "    \"\"\"Test complete analytics table build.\"\"\"\n",
    "    # Create minimal test data\n",
    "    test_orders = pd.DataFrame({\n",
    "        \"order_id\": [\"O1\", \"O2\", \"O3\"],\n",
    "        \"user_id\": [\"U1\", \"U2\", \"U1\"],\n",
    "        \"amount\": [100.0, 200.0, 5000.0],  # 5000 is an outlier\n",
    "        \"quantity\": [1, 2, 1],\n",
    "        \"created_at\": [\"2024-01-15 10:00:00\", \"2024-01-16 11:00:00\", \"2024-01-17 12:00:00\"],\n",
    "        \"status_clean\": [\"paid\", \"paid\", \"refund\"]\n",
    "    })\n",
    "\n",
    "    test_users = pd.DataFrame({\n",
    "        \"user_id\": [\"U1\", \"U2\"],\n",
    "        \"country\": [\"USA\", \"UK\"],\n",
    "        \"signup_date\": [\"2024-01-01\", \"2024-01-02\"]\n",
    "    })\n",
    "\n",
    "    result = (\n",
    "        test_orders\n",
    "        .pipe(parse_datetime, col=\"created_at\", utc=True)\n",
    "        .pipe(add_time_parts, ts_col=\"created_at\")\n",
    "        .pipe(safe_left_join, test_users, on=\"user_id\", validate=\"many_to_one\", suffixes=(\"\", \"_user\"))\n",
    "        .assign(amount_winsor=lambda d: winsorize(d[\"amount\"]))\n",
    "        .pipe(add_outlier_flag, col=\"amount\", k=1.5)\n",
    "    )\n",
    "\n",
    "    # Test 1: Row count preserved\n",
    "    assert len(result) == 3, \"Should have 3 rows\"\n",
    "\n",
    "    # Test 2: Time parts exist\n",
    "    assert all(col in result.columns for col in [\"month\", \"dow\", \"hour\"]), \\\n",
    "        \"Time parts should exist\"\n",
    "\n",
    "    # Test 3: User data joined\n",
    "    assert \"country\" in result.columns, \"Country should be joined\"\n",
    "    assert result[\"country\"].notna().all(), \"All users should match\"\n",
    "\n",
    "    # Test 4: Outlier flag exists\n",
    "    assert \"amount__is_outlier\" in result.columns, \"Outlier flag should exist\"\n",
    "\n",
    "    # Test 5: Outlier detected\n",
    "    assert result[\"amount__is_outlier\"].sum() >= 1, \"Should detect at least 1 outlier\"\n",
    "\n",
    "    print(\"‚úÖ test_analytics_table passed!\")\n",
    "\n",
    "test_analytics_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88383150",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary & Final Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f8508",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Datetime Parsing**: Always use `errors=\"coerce\"` to handle invalid dates safely\n",
    "2. **Time Parts**: Add grouping keys (month, dow, hour) for easy aggregation\n",
    "3. **Outliers**: Use IQR bounds to flag, winsorize for visualization (don't delete!)\n",
    "4. **Core Pandas**: `.loc`, `.assign`, `groupby/agg` are your daily workhorses\n",
    "5. **Safe Joins**: Use `validate=` to prevent explosions, check uniqueness first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a314d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run All Tests\n",
    "_cell(\"final_verification\")\n",
    "\n",
    "# Run all tests to verify your understanding\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING ALL UNIT TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tests = [\n",
    "    (\"parse_datetime\", test_parse_datetime),\n",
    "    (\"add_time_parts\", test_add_time_parts),\n",
    "    (\"percentiles\", test_percentiles),\n",
    "    (\"iqr_bounds\", test_iqr_bounds),\n",
    "    (\"winsorize\", test_winsorize),\n",
    "    (\"filter_paid\", test_filter_paid),\n",
    "    (\"add_boolean\", test_add_boolean),\n",
    "    (\"revenue_per_user\", test_revenue_per_user),\n",
    "    (\"refund_rate\", test_refund_rate),\n",
    "    (\"safe_join\", test_safe_join),\n",
    "    (\"dtype_match\", test_dtype_match),\n",
    "    (\"analytics_table\", test_analytics_table),\n",
    "]\n",
    "\n",
    "passed = 0\n",
    "failed = 0\n",
    "\n",
    "for name, test_func in tests:\n",
    "    try:\n",
    "        test_func()\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {name} FAILED: {e}\")\n",
    "        failed += 1\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(f\"RESULTS: {passed} passed, {failed} failed\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if failed == 0:\n",
    "    print(\"\\nüéâ Congratulations! All tests passed!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è {failed} test(s) need attention. Review the exercises above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af02996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Final Summary\n",
    "_cell(\"final_summary\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Student: {STUDENT_NAME}\")\n",
    "print(f\"Session: {_SESS}\")\n",
    "print(f\"Cells run: {_cell_num}\")\n",
    "print(f\"Exercises attempted: {len(_attempts)}\")\n",
    "if _attempts:\n",
    "    for ex, tries in sorted(_attempts.items()):\n",
    "        print(f\"   - {ex}: {tries} attempt(s)\")\n",
    "print()\n",
    "_capture(\"summary\", {\"cells\": _cell_num, \"exercises\": len(_attempts)})\n",
    "print(\"Great work!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
